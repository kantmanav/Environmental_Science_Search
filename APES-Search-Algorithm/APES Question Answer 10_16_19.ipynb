{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'What is the change in the genetic composition of a population over time?'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "#Will implement entity list in the future\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "#Will implement function to standardize proximity scores in the future\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '11',\n",
       " '_score': 8.522448,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 11,\n",
       "  'content': 'Earth’s biodiversity is the product of evolution,  which can be defined as a change in the genetic composition of a population over time. Evolution can  occur at multiple levels. Evolution below the species  level, such as the evolution of different varieties of  apples or potatoes, is called microevolution. In contrast, when genetic changes give rise to new species, or  to new genera, families, classes, or phyla—larger categories of organisms into which species are organized—  we call the process macroevolution. Among these  many levels of macroevolution, the term speciation is  restricted to the evolution of new species.'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'The number of species in a given area is known as what?'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "#Will implement entity list in the future\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "#Will implement function to standardize proximity scores in the future\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '6',\n",
       " '_score': 4.1437445,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 6,\n",
       "  'content': 'The number of species in a given area, such as a  pond, the canopy of a tree, or a plot of grassland, is  known as species richness. Species richness is used  to give an approximate sense of the biodiversity of a  particular place. However, we may also want to know  the species evenness, which is the relative proportion of individuals within the different species in a  location. Species evenness tells us whether a particular  ecosystem is numerically dominated by one species or  whether all of its species have similar abundances. An  ecosystem has high species evenness if its species are  all represented by similar numbers of individuals. An  ecosystem has low species evenness if one species is  represented by many individuals whereas other species are represented by only a few individuals. In this  case, there is effectively less diversity.'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'This is a result of the phenomenon resulting in catastrophic failure.'\n",
    "doc = nlp(s)\n",
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is a result of the phenomenon resulting in catastrophic failure."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'result',\n",
       " 'of',\n",
       " 'the',\n",
       " 'phenomenon',\n",
       " 'resulting',\n",
       " 'in',\n",
       " 'catastrophic',\n",
       " 'failure',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.ent_type for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('result', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('phenomenon', 'NOUN'),\n",
       " ('resulting', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('catastrophic', 'ADJ'),\n",
       " ('failure', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Phenomena resulting in catastrophic failure.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DET', 'this'),\n",
       " ('is', 'VERB', 'be'),\n",
       " ('a', 'DET', 'a'),\n",
       " ('result', 'NOUN', 'result'),\n",
       " ('of', 'ADP', 'of'),\n",
       " ('the', 'DET', 'the'),\n",
       " ('phenomenon', 'NOUN', 'phenomenon'),\n",
       " ('resulting', 'VERB', 'result'),\n",
       " ('in', 'ADP', 'in'),\n",
       " ('catastrophic', 'ADJ', 'catastrophic'),\n",
       " ('failure', 'NOUN', 'failure'),\n",
       " ('.', 'PUNCT', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.pos_, token.lemma_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'What is the change in the genetic composition of a population over time?'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "#Will implement entity list in the future\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "#Will implement function to standardize proximity scores in the future\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '11',\n",
       " '_score': 8.522448,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 11,\n",
       "  'content': 'Earth’s biodiversity is the product of evolution,  which can be defined as a change in the genetic composition of a population over time. Evolution can  occur at multiple levels. Evolution below the species  level, such as the evolution of different varieties of  apples or potatoes, is called microevolution. In contrast, when genetic changes give rise to new species, or  to new genera, families, classes, or phyla—larger categories of organisms into which species are organized—  we call the process macroevolution. Among these  many levels of macroevolution, the term speciation is  restricted to the evolution of new species.'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'Artificial selection proceeds at a much quicker rate than natural selection'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "#Will implement entity list in the future\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "#Will implement function to standardize proximity scores in the future\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '30',\n",
       " '_score': 7.964067,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 30,\n",
       "  'content': 'Artificial and natural selection are important mechanisms of evolution, but evolution can also occur by  random, or nonadaptive, processes. In these cases, the  genetic composition of a population changes over  time, but the changes are not related to differences in  fitness among individuals. There are five random processes: mutation, gene flow, genetic drift, bottleneck effects,  and founder effects.'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'Artificial selection natural selection quicker'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "#Will implement entity list in the future\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "#Will implement function to standardize proximity scores in the future\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '30',\n",
       " '_score': 7.964067,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 30,\n",
       "  'content': 'Artificial and natural selection are important mechanisms of evolution, but evolution can also occur by  random, or nonadaptive, processes. In these cases, the  genetic composition of a population changes over  time, but the changes are not related to differences in  fitness among individuals. There are five random processes: mutation, gene flow, genetic drift, bottleneck effects,  and founder effects.'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'Artificial selection quicker'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "#Will implement entity list in the future\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "#Will implement function to standardize proximity scores in the future\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '21',\n",
       " '_score': 5.0261393,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 21,\n",
       "  'content': 'When humans determine which individuals to  breed, typically with a preconceived set of traits in  mind, we call the process evolution by artificial  selection. Artificial selection has produced numerous  breeds of horses, cattle, sheep, pigs, and chickens with  traits that humans find useful or aesthetically pleasing.  Most of our modern agricultural crops are also the  result of many years of careful breeding. For example,  starting with a single species of wild mustard, Brassica  oleracea, plant breeders have produced a variety of food crops, including cabbage, cauliflower, broccoli, Brussels  sprouts, kale, and kohlrabi.'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "ConnectionError(<urllib3.connection.HTTPConnection object at 0x1217130f0>: Failed to establish a new connection: [Errno 61] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x1217130f0>: Failed to establish a new connection: [Errno 61] Connection refused)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    231\u001b[0m             response = self.pool.urlopen(\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRetry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             )\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Disabled, indicate to re-raise the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 168\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x1217130f0>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6335a022db26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             'must': [{\n\u001b[1;32m     24\u001b[0m                 'match': {\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 }\n\u001b[1;32m     27\u001b[0m             }]\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/elasticsearch/client/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, index, body, params)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"_all\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         return self.transport.perform_request(\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_search\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                 )\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/anaconda3/anaconda3/lib/python3.7/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TIMEOUT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"N/A\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# raise errors based on http status codes, let the client handle those if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ConnectionError(<urllib3.connection.HTTPConnection object at 0x1217130f0>: Failed to establish a new connection: [Errno 61] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x1217130f0>: Failed to establish a new connection: [Errno 61] Connection refused)"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'adaptaion mutation'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "#Will implement entity list in the future\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "#Will implement function to standardize proximity scores in the future\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'The number of species in a given area is known as'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "#Will implement entity list in the future\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "#Will implement function to standardize proximity scores in the future\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '6',\n",
       " '_score': 4.1437445,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 6,\n",
       "  'content': 'The number of species in a given area, such as a  pond, the canopy of a tree, or a plot of grassland, is  known as species richness. Species richness is used  to give an approximate sense of the biodiversity of a  particular place. However, we may also want to know  the species evenness, which is the relative proportion of individuals within the different species in a  location. Species evenness tells us whether a particular  ecosystem is numerically dominated by one species or  whether all of its species have similar abundances. An  ecosystem has high species evenness if its species are  all represented by similar numbers of individuals. An  ecosystem has low species evenness if one species is  represented by many individuals whereas other species are represented by only a few individuals. In this  case, there is effectively less diversity.'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'The range of abiotic and biotic conditions under which a species actually lives is called its '\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '52',\n",
       " '_score': 13.8511095,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 52,\n",
       "  'content': 'The fundamental niche establishes the abiotic  limits for the persistence of a species. However,  biotic factors can further limit the locations where a  species can live. Common biotic limitations include  the presence of competitors, predators, and diseases.  For example, even if abiotic conditions are favorable  for a plant species in a particular location, other  plant species may be better competitors for water  and soil nutrients. Those competitors might prevent  the species from growing in that environment.  Similarly, even if a small rodent can tolerate the  temperature and humidity of a tropical forest, a  deadly rodent disease might prevent the species  from persisting in the forest. Therefore, biotic factors further narrow the fundamental niche that a  species actually uses. The range of abiotic and biotic  conditions under which a species actually lives is  called its realized niche. Once we determine what  contributes to the realized niche of a species, we  have a better understanding of the distribution of  the species, or the areas of the world in which the  species lives.'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'When humans determine which individuals breed, we call this'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '21',\n",
       " '_score': 4.2195663,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 21,\n",
       "  'content': 'When humans determine which individuals to  breed, typically with a preconceived set of traits in  mind, we call the process evolution by artificial  selection. Artificial selection has produced numerous  breeds of horses, cattle, sheep, pigs, and chickens with  traits that humans find useful or aesthetically pleasing.  Most of our modern agricultural crops are also the  result of many years of careful breeding. For example,  starting with a single species of wild mustard, Brassica  oleracea, plant breeders have produced a variety of food crops, including cabbage, cauliflower, broccoli, Brussels  sprouts, kale, and kohlrabi.'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'When humans determine which individuals breed, we call this'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '21',\n",
       " '_score': 4.2195663,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 21,\n",
       "  'content': 'When humans determine which individuals to  breed, typically with a preconceived set of traits in  mind, we call the process evolution by artificial  selection. Artificial selection has produced numerous  breeds of horses, cattle, sheep, pigs, and chickens with  traits that humans find useful or aesthetically pleasing.  Most of our modern agricultural crops are also the  result of many years of careful breeding. For example,  starting with a single species of wild mustard, Brassica  oleracea, plant breeders have produced a variety of food crops, including cabbage, cauliflower, broccoli, Brussels  sprouts, kale, and kohlrabi.'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'When humans determine which individuals breed, we call this'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '21',\n",
       " '_score': 4.2195663,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 21,\n",
       "  'content': 'When humans determine which individuals to  breed, typically with a preconceived set of traits in  mind, we call the process evolution by artificial  selection. Artificial selection has produced numerous  breeds of horses, cattle, sheep, pigs, and chickens with  traits that humans find useful or aesthetically pleasing.  Most of our modern agricultural crops are also the  result of many years of careful breeding. For example,  starting with a single species of wild mustard, Brassica  oleracea, plant breeders have produced a variety of food crops, including cabbage, cauliflower, broccoli, Brussels  sprouts, kale, and kohlrabi.'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'When humans determine which individuals breed, we call this'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = 'When humans determine which individuals breed, we call this'\n",
    "question = nlp(s)\n",
    "\n",
    "search_ents = []\n",
    "for token in question:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'ADJ':\n",
    "        search_ents.append(token)\n",
    "        \n",
    "search_ents_txt = [token.text for token in search_ents]\n",
    "content = ' '.join(search_ents_txt)\n",
    "        \n",
    "res = es.search(index = 'chapter5', body = {\n",
    "    'query': {\n",
    "        'bool': {\n",
    "            'must': [{\n",
    "                'match': {\n",
    "                    'content': content\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "doc_hits = res['hits']['hits']\n",
    "pars = [nlp(doc['_source']['content']) for doc in res['hits']['hits']]\n",
    "\n",
    "def minimum_dist(a, b):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return 100\n",
    "    a_idx = 0\n",
    "    b_idx = 0\n",
    "    dist = abs(a[0] - b[0])\n",
    "    while (a_idx < len(a) and b_idx < len(b)):\n",
    "        if abs(a[a_idx] - b[b_idx]) < dist:\n",
    "            dist = abs(a[a_idx] - b[b_idx])\n",
    "        if a[a_idx] < b[b_idx]:\n",
    "            a_idx += 1\n",
    "        else:\n",
    "            b_idx += 1\n",
    "    return dist\n",
    "\n",
    "def prox_sc(ent_pos_list):\n",
    "    dist_sum = 0\n",
    "    for i in range(len(ent_pos_list) - 1):\n",
    "        for k in range(i + 1, len(ent_pos_list)):\n",
    "            dist_sum += minimum_dist(ent_pos_list[i], ent_pos_list[k])\n",
    "    dist_avg = float(dist_sum) / float(len(ent_pos_list))\n",
    "    return dist_avg\n",
    "\n",
    "prox_scores = []\n",
    "for par in pars:\n",
    "    ent_positions = []\n",
    "    for i in range(len(search_ents)):\n",
    "        ent_positions.append([token.i for token in par if token.text == search_ents_txt[i]])\n",
    "    prox_scores.append(prox_sc(ent_positions))\n",
    "    \n",
    "best_hit = prox_scores.index(min(prox_scores))\n",
    "best_par = doc_hits[best_hit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'chapter5',\n",
       " '_type': '_doc',\n",
       " '_id': '21',\n",
       " '_score': 4.2195663,\n",
       " '_source': {'chapter': 5,\n",
       "  'paragraph': 21,\n",
       "  'content': 'When humans determine which individuals to  breed, typically with a preconceived set of traits in  mind, we call the process evolution by artificial  selection. Artificial selection has produced numerous  breeds of horses, cattle, sheep, pigs, and chickens with  traits that humans find useful or aesthetically pleasing.  Most of our modern agricultural crops are also the  result of many years of careful breeding. For example,  starting with a single species of wild mustard, Brassica  oleracea, plant breeders have produced a variety of food crops, including cabbage, cauliflower, broccoli, Brussels  sprouts, kale, and kohlrabi.'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
